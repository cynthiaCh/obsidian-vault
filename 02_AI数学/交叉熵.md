# 交叉熵（Cross Entropy）

衡量预测概率分布与真实分布之间差异的函数。

## 公式
H(y, p) = -∑ yᵢ * log(pᵢ)

## 通俗理解
- 猜得越准，loss 越小
- 猜得越离谱，loss 越大

Softmax 输出的概率作为输入。

参考笔记：[[Softmax]]
